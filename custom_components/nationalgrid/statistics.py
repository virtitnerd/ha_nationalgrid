"""
Import AMI energy data into Home Assistant long-term statistics.

Creates two external statistic series per electric meter and one per gas meter:

- ``nationalgrid:{service_point}_hourly_usage`` — daily AMI readings (electric kWh,
  gas CCF converted from therms) imported as hourly statistics.
- ``nationalgrid:{service_point}_interval_usage`` — 15-minute interval reads
  (electric only, kWh) bucketed into hourly totals.

Statistics are imported incrementally: on each coordinator update, only readings
newer than the last imported timestamp are appended. Cumulative sums are carried
forward from the previous import.

Note: HA long-term statistics require top-of-hour timestamps. AMI timestamps are
truncated to the hour, and interval reads are aggregated into hourly buckets.
"""

from __future__ import annotations

from datetime import UTC, datetime
from functools import partial
from typing import TYPE_CHECKING

from homeassistant.components.recorder import get_instance
from homeassistant.components.recorder.models import StatisticData, StatisticMetaData
from homeassistant.components.recorder.statistics import (
    async_add_external_statistics,
    get_last_statistics,
)
from homeassistant.const import UnitOfEnergy

from .const import DOMAIN, LOGGER, therms_to_ccf

if TYPE_CHECKING:
    from homeassistant.core import HomeAssistant

    from .coordinator import NationalGridDataUpdateCoordinator


async def async_import_all_statistics(
    hass: HomeAssistant,
    coordinator: NationalGridDataUpdateCoordinator,
) -> None:
    """
    Import AMI hourly and interval read statistics.

    Called after the first coordinator refresh and on each subsequent update.
    Iterates all AMI meters and electric interval reads, delegating to the
    appropriate import function for each.
    """
    data = coordinator.data
    if data is None:
        return

    for sp, ami_readings in data.ami_usages.items():
        meter_data = data.meters.get(sp)
        if meter_data is None:
            continue
        fuel_type = str(meter_data.meter.get("fuelType", ""))
        is_gas = fuel_type == "Gas"

        # Import hourly AMI stats as external statistics
        await _import_hourly_stats(hass, sp, ami_readings, is_gas=is_gas)

    # Import interval read stats (electric only)
    for sp, reads in data.interval_reads.items():
        await _import_interval_stats(hass, sp, reads)


async def _import_hourly_stats(
    hass: HomeAssistant,
    service_point: str,
    readings: list,
    *,
    is_gas: bool,
) -> None:
    """
    Import hourly AMI usage statistics as external statistics.

    Each AMI reading has a date and quantity (in therms for gas, kWh for electric).
    Gas quantities are converted to CCF. Readings are sorted chronologically and
    only those newer than the last imported statistic are appended.
    """
    statistic_id = f"{DOMAIN}:{service_point}_hourly_usage"
    unit = "CCF" if is_gas else UnitOfEnergy.KILO_WATT_HOUR

    # Get last imported sum to continue cumulative total
    last = await get_instance(hass).async_add_executor_job(
        partial(
            get_last_statistics,
            hass,
            1,
            statistic_id,
            convert_units=True,
            types={"sum"},
        )
    )
    last_sum = 0.0
    last_ts = 0.0
    if last.get(statistic_id):
        row = last[statistic_id][0]
        last_sum = row.get("sum") or 0.0
        last_ts = row.get("start") or 0.0

    # Sort readings by date and filter to new ones only
    sorted_readings = sorted(readings, key=lambda r: str(r.get("date", "")))
    stats: list[StatisticData] = []
    running_sum = last_sum

    for reading in sorted_readings:
        date_str = str(reading.get("date", ""))
        quantity = float(reading.get("quantity", 0))
        if not date_str:
            continue

        # Parse date string (ISO 8601 format, e.g. "2026-01-22T15:00:00.000Z")
        try:
            dt = datetime.fromisoformat(date_str)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=UTC)
            # Truncate to top of hour for HA statistics
            dt = dt.replace(minute=0, second=0, microsecond=0)
        except ValueError:
            LOGGER.debug("Could not parse AMI date: %s", date_str)
            continue

        if dt.timestamp() <= last_ts:
            continue

        value = therms_to_ccf(quantity) if is_gas else quantity
        running_sum += value
        stats.append(
            StatisticData(
                start=dt,
                state=value,
                sum=running_sum,
            )
        )

    if not stats:
        return

    metadata = StatisticMetaData(
        has_mean=False,
        has_sum=True,
        name=f"{service_point} Hourly Usage",
        source=DOMAIN,
        statistic_id=statistic_id,
        unit_of_measurement=unit,
    )

    async_add_external_statistics(hass, metadata, stats)
    LOGGER.debug(
        "Imported %s hourly stats for %s (sum=%.3f)",
        len(stats),
        statistic_id,
        running_sum,
    )


async def _import_interval_stats(
    hass: HomeAssistant,
    service_point: str,
    reads: list,
) -> None:
    """
    Import 15-minute interval read statistics for a service point.

    Interval reads arrive at 15-minute granularity but HA statistics require
    hourly timestamps. Reads are aggregated (summed) into hourly buckets before
    import. Only available for electric meters.
    """
    statistic_id = f"{DOMAIN}:{service_point}_interval_usage"

    last = await get_instance(hass).async_add_executor_job(
        partial(
            get_last_statistics,
            hass,
            1,
            statistic_id,
            convert_units=True,
            types={"sum"},
        )
    )
    last_sum = 0.0
    last_ts = 0.0
    if last.get(statistic_id):
        row = last[statistic_id][0]
        last_sum = row.get("sum") or 0.0
        last_ts = row.get("start") or 0.0

    # Bucket interval reads by hour (HA requires top-of-hour timestamps)
    hourly_buckets: dict[datetime, float] = {}
    for read in reads:
        start_str = str(read.get("startTime", ""))
        value = float(read.get("value", 0))
        if not start_str:
            continue

        try:
            dt = datetime.fromisoformat(start_str)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=UTC)
        except ValueError:
            LOGGER.debug("Could not parse interval startTime: %s", start_str)
            continue

        hour_start = dt.replace(minute=0, second=0, microsecond=0)
        hourly_buckets[hour_start] = hourly_buckets.get(hour_start, 0.0) + value

    stats: list[StatisticData] = []
    running_sum = last_sum

    for hour_start in sorted(hourly_buckets):
        if hour_start.timestamp() <= last_ts:
            continue

        hour_total = hourly_buckets[hour_start]
        running_sum += hour_total
        stats.append(
            StatisticData(
                start=hour_start,
                state=hour_total,
                sum=running_sum,
            )
        )

    if not stats:
        return

    metadata = StatisticMetaData(
        has_mean=False,
        has_sum=True,
        name=f"{service_point} Interval Usage",
        source=DOMAIN,
        statistic_id=statistic_id,
        unit_of_measurement=UnitOfEnergy.KILO_WATT_HOUR,
    )

    async_add_external_statistics(hass, metadata, stats)
    LOGGER.debug(
        "Imported %s interval stats for %s (sum=%.3f)",
        len(stats),
        service_point,
        running_sum,
    )
